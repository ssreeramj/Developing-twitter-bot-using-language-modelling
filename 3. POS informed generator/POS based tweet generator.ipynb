{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import string\n",
    "import random\n",
    "\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH = 'corpus.txt'\n",
    "\n",
    "stopwords = ['', '(', ')', '{', '}', '\\\\', '--', ':', '-', \"'s\"]\n",
    "punc = string.punctuation + \"``\" + \"''\" + '\"'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(FILE_PATH, 'r') as f:\n",
    "    data = f.read().lower().replace('\\n',' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3954021"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenized_words(data):\n",
    "    sents = []\n",
    "    for sent in nltk.sent_tokenize(data):\n",
    "        words = [word for word in nltk.word_tokenize(sent) if word not in stopwords and word not in punc]\n",
    "        sents.append(words)\n",
    "        \n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenized_rev_words(data):\n",
    "    sents = []\n",
    "    for sent in nltk.sent_tokenize(data):\n",
    "        words = [word for word in nltk.word_tokenize(sent) if word not in stopwords and word not in punc]\n",
    "        sents.append(list(reversed(words)))\n",
    "        \n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = tokenized_words(data)\n",
    "rev_sents = tokenized_rev_words(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40992"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus = [word for sent in sents for word in sent]\n",
    "rev_train_corpus = [word for sent in rev_sents for word in sent] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "666665"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_freq_dist(corpus, ngram=1):\n",
    "    if isinstance(corpus, list) and len(corpus)>0:\n",
    "        train_corpus=corpus\n",
    "    elif type(corpus) is str:\n",
    "        train_corpus=nltk.word_tokenize(corpus)\n",
    "    else:\n",
    "        print('Error')\n",
    "        return None\n",
    "    \n",
    "    freq_dist=None\n",
    "    if ngram==1:\n",
    "        freq_dist = nltk.FreqDist(train_corpus) #freq distibution for unigrams\n",
    "    elif ngram==2:\n",
    "        freq_dist = nltk.ConditionalFreqDist(nltk.ngrams(train_corpus, 2))# conditional freq dist for bigrams\n",
    "    elif ngram==3:\n",
    "        trigrams_as_bigrams=[]\n",
    "        trigram =[a for a in ngrams(train_corpus, 3)]\n",
    "        trigrams_as_bigrams.extend([((t[0],t[1]), t[2]) for t in trigram])\n",
    "        freq_dist = nltk.ConditionalFreqDist(trigrams_as_bigrams)# conditional freq dist for trigrams\n",
    "    else:\n",
    "        print('Supported upto trigrams only')\n",
    "    return freq_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfd_2gram = ngram_freq_dist(train_corpus, 2)\n",
    "cfd_2gram_rev = ngram_freq_dist(rev_train_corpus, 2)\n",
    "\n",
    "cpd_2gram = nltk.ConditionalProbDist(cfd_2gram, nltk.MLEProbDist)\n",
    "cpd_2gram_rev = nltk.ConditionalProbDist(cfd_2gram_rev, nltk.MLEProbDist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to generate random tweets\n",
    "\n",
    "def generate_txt_bigram_model_random(cprob_2gram, cprob_2gram_rev, initialword, numwords=15):\n",
    "    text = initialword\n",
    "    suf_word = initialword\n",
    "    pre_word = initialword\n",
    "    for index in range(numwords):\n",
    "        if random.random() > 0.5:\n",
    "            try:\n",
    "                suf_word = cprob_2gram[suf_word].generate()\n",
    "                text = text + \" \" + suf_word\n",
    "            except Exception as e:\n",
    "                print('Can not generate the sentence')\n",
    "                return\n",
    "        else:\n",
    "            try: \n",
    "                pre_word = cprob_2gram_rev[pre_word].generate()\n",
    "                text = pre_word + ' ' + text\n",
    "            except Exception as e:\n",
    "                print('Can not generate the sentence')\n",
    "                return\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_sentences = []\n",
    "random_pos_tags = []\n",
    "random_word_pos_tags = []\n",
    "\n",
    "# Generate 200 sentences randomly \n",
    "for _ in range(5000):\n",
    "    sent = generate_txt_bigram_model_random(cpd_2gram, cpd_2gram_rev, 'election', 9)\n",
    "    word_pos_tags = nltk.pos_tag(sent.split())\n",
    "    pos_tags = [x[1] for x in word_pos_tags]\n",
    "    \n",
    "    random_word_pos_tags.append(word_pos_tags)\n",
    "    random_sentences.append(sent)\n",
    "    random_pos_tags.append(pos_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "RULES:\n",
    "\n",
    "1. Determiner always comes before a noun.\n",
    "2. Noun can be followed by another noun phrase.\n",
    "3. Modals (could, will) can follow nouns.\n",
    "4. ..\n",
    "\n",
    "'''\n",
    "\n",
    "pos_template_dict = {\n",
    "    'NN': ['NN', 'VB', 'VBD', 'MD', 'VBP', 'IN', 'VBZ', 'NNS'],\n",
    "    'NNS': ['NN', 'VB', 'VBD', 'MD', 'VBP', 'IN', 'NN'],\n",
    "    'NNP': ['NN', 'VB', 'VBD', 'MD', 'VBP', 'IN', 'NNS'],\n",
    "    'NNPS': ['NN', 'VB', 'VBD', 'MD', 'VBP', 'IN'],\n",
    "    'DT': ['NN', 'NNS', 'NNP', 'NNPS', 'VBP', 'JJ'],\n",
    "    'JJ': ['CC'],\n",
    "    'CC': ['NN', 'NNS', 'NNP', 'NNPS'],\n",
    "    'VB': ['NN', 'DT', 'TO'],\n",
    "    'VBD': ['NN', 'TO'],\n",
    "    'VBG': ['IN', 'TO'],\n",
    "    'VBP': ['VBG', 'RB', 'TO'],\n",
    "    'VBN': ['RB', 'PRP', 'TO'],\n",
    "    'VBZ': ['VBN'],\n",
    "    'MD': ['VB', 'PRP'],\n",
    "    'IN': ['DT', 'JJ'],\n",
    "    'RB': ['NN', 'NNS'],\n",
    "    'PRP': ['MD', 'VBD'],\n",
    "    'TO': ['VB'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code to accept sentences which match the POS template\n",
    "def filter_sentences(postags_list, sent_list, template):\n",
    "    filtered_sent = []\n",
    "    for ind, pos_tag in enumerate(postags_list):\n",
    "        if is_pos_tag_match(pos_tag, template):\n",
    "            filtered_sent.append(sent_list[ind])\n",
    "            \n",
    "    return filtered_sent\n",
    "        \n",
    "        \n",
    "def is_pos_tag_match(tag, template):\n",
    "    start = tag[0]\n",
    "    if start in template:\n",
    "        for t in tag[1:]:\n",
    "            if t not in template[start]:\n",
    "                return False\n",
    "            else:\n",
    "                start = t\n",
    "        else:\n",
    "            return True\n",
    "    return False\n",
    "    \n",
    "def print_filtered_sent(filt_sent):\n",
    "    for sent in filt_sent:\n",
    "        print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Results</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "than this election day dinner rt nytopinion estoy listo para\n",
      "official i hope to create an election days excited to\n",
      "a number of the election day of the rest of\n",
      "sanders on this election must be a smartphone “ role\n",
      "s must end election —hillary on the gourmet goat in\n",
      "in this election day rt mccraylaurie martin luther king jr.\n",
      "face on the basics of this election join the only\n",
      "be a reminder of this election cycle than all make\n",
      "address the people in this election results have to stand\n",
      "the atrocity in the election day would provide food –\n",
      "con kaine connects in this election of the fact mainstream\n",
      "live in this election days grateful to caucus campaign stop\n",
      "coming to move this election for the michigandems reception education\n",
      "editorial in the lives for the election in a hillary\n"
     ]
    }
   ],
   "source": [
    "print_filtered_sent(filter_sentences(random_pos_tags, random_sentences, pos_template_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
