{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import string\n",
    "import random\n",
    "\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH = 'corpus.txt'\n",
    "\n",
    "stopwords = ['', '(', ')', '{', '}', '\\\\', '--', ':', '-', \"'s\"]\n",
    "punc = string.punctuation + \"``\" + \"''\" + '\"'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(FILE_PATH, 'r') as f:\n",
    "    data = f.read().lower().replace('\\n',' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3954021"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenized_words(data):\n",
    "    sents = []\n",
    "    for sent in nltk.sent_tokenize(data):\n",
    "        words = [word for word in nltk.word_tokenize(sent) if word not in stopwords and word not in punc]\n",
    "        sents.append(words)\n",
    "        \n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenized_rev_words(data):\n",
    "    sents = []\n",
    "    for sent in nltk.sent_tokenize(data):\n",
    "        words = [word for word in nltk.word_tokenize(sent) if word not in stopwords and word not in punc]\n",
    "        sents.append(list(reversed(words)))\n",
    "        \n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = tokenized_words(data)\n",
    "rev_sents = tokenized_rev_words(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40992"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus = [word for sent in sents for word in sent]\n",
    "rev_train_corpus = [word for sent in rev_sents for word in sent] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "666665"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_freq_dist(corpus, ngram=1):\n",
    "    if isinstance(corpus, list) and len(corpus)>0:\n",
    "        train_corpus=corpus\n",
    "    elif type(corpus) is str:\n",
    "        train_corpus=nltk.word_tokenize(corpus)\n",
    "    else:\n",
    "        print('Error')\n",
    "        return None\n",
    "    \n",
    "    freq_dist=None\n",
    "    if ngram==1:\n",
    "        freq_dist = nltk.FreqDist(train_corpus) #freq distibution for unigrams\n",
    "    elif ngram==2:\n",
    "        freq_dist = nltk.ConditionalFreqDist(nltk.ngrams(train_corpus, 2))# conditional freq dist for bigrams\n",
    "    elif ngram==3:\n",
    "        trigrams_as_bigrams=[]\n",
    "        trigram =[a for a in ngrams(train_corpus, 3)]\n",
    "        trigrams_as_bigrams.extend([((t[0],t[1]), t[2]) for t in trigram])\n",
    "        freq_dist = nltk.ConditionalFreqDist(trigrams_as_bigrams)# conditional freq dist for trigrams\n",
    "    else:\n",
    "        print('Supported upto trigrams only')\n",
    "    return freq_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfd_2gram = ngram_freq_dist(train_corpus, 2)\n",
    "cfd_2gram_rev = ngram_freq_dist(rev_train_corpus, 2)\n",
    "\n",
    "cpd_2gram = nltk.ConditionalProbDist(cfd_2gram, nltk.MLEProbDist)\n",
    "cpd_2gram_rev = nltk.ConditionalProbDist(cfd_2gram_rev, nltk.MLEProbDist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfd_1gram = ngram_freq_dist(train_corpus)\n",
    "cpd_1gram = nltk.MLEProbDist(cfd_1gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to generate random tweets\n",
    "\n",
    "def generate_txt_bigram_model_random(cprob_2gram, cprob_2gram_rev, initialword, numwords=15):\n",
    "    text = initialword\n",
    "    suf_word = initialword\n",
    "    pre_word = initialword\n",
    "    for index in range(numwords):\n",
    "        if random.random() > 0.5:\n",
    "            try:\n",
    "                suf_word = cprob_2gram[suf_word].generate()\n",
    "                text = text + \" \" + suf_word\n",
    "            except Exception as e:\n",
    "                print('Can not generate the sentence')\n",
    "                return\n",
    "        else:\n",
    "            try: \n",
    "                pre_word = cprob_2gram_rev[pre_word].generate()\n",
    "                text = pre_word + ' ' + text\n",
    "            except Exception as e:\n",
    "                print('Can not generate the sentence')\n",
    "                return\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_sentences = []\n",
    "random_pos_tags = []\n",
    "random_word_pos_tags = []\n",
    "\n",
    "# Generate 5000 sentences randomly \n",
    "for _ in range(5000):\n",
    "    sent = generate_txt_bigram_model_random(cpd_2gram, cpd_2gram_rev, 'election', 9)\n",
    "    word_pos_tags = nltk.pos_tag(sent.split())\n",
    "    pos_tags = [x[1] for x in word_pos_tags]\n",
    "    \n",
    "    random_word_pos_tags.append(word_pos_tags)\n",
    "    random_sentences.append(sent)\n",
    "    random_pos_tags.append(pos_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "RULES:\n",
    "\n",
    "1. Determiner always comes before a noun.\n",
    "2. Noun can be followed by another noun phrase.\n",
    "3. Modals (could, will) can follow nouns.\n",
    "4. ..\n",
    "\n",
    "'''\n",
    "\n",
    "pos_template_dict = {\n",
    "    'NN': ['NN', 'VB', 'VBD', 'MD', 'VBP', 'IN', 'VBZ', 'NNS'],\n",
    "    'NNS': ['NN', 'VB', 'VBD', 'MD', 'VBP', 'IN', 'NN'],\n",
    "    'NNP': ['NN', 'VB', 'VBD', 'MD', 'VBP', 'IN', 'NNS'],\n",
    "    'NNPS': ['NN', 'VB', 'VBD', 'MD', 'VBP', 'IN'],\n",
    "    'DT': ['NN', 'NNS', 'NNP', 'NNPS', 'VBP', 'JJ'],\n",
    "    'JJ': ['CC'],\n",
    "    'CC': ['NN', 'NNS', 'NNP', 'NNPS'],\n",
    "    'VB': ['NN', 'DT', 'TO'],\n",
    "    'VBD': ['NN', 'TO'],\n",
    "    'VBG': ['IN', 'TO'],\n",
    "    'VBP': ['VBG', 'RB', 'TO'],\n",
    "    'VBN': ['RB', 'PRP', 'TO'],\n",
    "    'VBZ': ['VBN'],\n",
    "    'MD': ['VB', 'PRP'],\n",
    "    'IN': ['DT', 'JJ'],\n",
    "    'RB': ['NN', 'NNS'],\n",
    "    'PRP': ['MD', 'VBD'],\n",
    "    'TO': ['VB'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code to accept sentences which match the POS template\n",
    "def filter_sentences(postags_list, sent_list, template):\n",
    "    filtered_sent = []\n",
    "    for ind, pos_tag in enumerate(postags_list):\n",
    "        if is_pos_tag_match(pos_tag, template):\n",
    "            filtered_sent.append(sent_list[ind])\n",
    "            \n",
    "    return filtered_sent\n",
    "        \n",
    "        \n",
    "def is_pos_tag_match(tag, template):\n",
    "    start = tag[0]\n",
    "    if start in template:\n",
    "        for t in tag[1:]:\n",
    "            if t not in template[start]:\n",
    "                return False\n",
    "            else:\n",
    "                start = t\n",
    "        else:\n",
    "            return True\n",
    "    return False\n",
    "    \n",
    "def print_filtered_sent(filt_sent):\n",
    "    for sent in filt_sent:\n",
    "        print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Results</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at the discipline temperament knowledge on this election day …\n",
      "zone in this election day rt businessinsider americans age of\n",
      "this election cycle … i got to look fact did\n",
      "earn the people like this election cycle of the greed\n",
      "this afternoon phone bank on this election days should be\n",
      "confirmation process as a party leadership in this election is\n",
      "government must end election in early and share yours have\n",
      "force against this election days left to be a trump\n",
      "a country can fix this election will make the us-mexico\n",
      "to solve this election join forces that some questions at\n",
      "this election days left to run interference from the wealthy\n"
     ]
    }
   ],
   "source": [
    "filtered_sent = filter_sentences(random_pos_tags, random_sentences, pos_template_dict)\n",
    "print_filtered_sent(filtered_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_prob(sent_list, cpd_1gram, cpd_2gram):\n",
    "    sent_prob_dict = {}\n",
    "    for sent in sent_list:\n",
    "        total_prob = 1.0\n",
    "        words = nltk.word_tokenize(sent)\n",
    "        total_prob = cpd_1gram.prob(words[0])\n",
    "        for w1, w2 in nltk.ngrams(words, 2):\n",
    "            total_prob *= cpd_2gram[w1].prob(w2)\n",
    "        sent_prob_dict[sent] = total_prob\n",
    "    return sent_prob_dict\n",
    "\n",
    "def get_top_five(dict_of_probs):\n",
    "    sorted_dict = sorted(dict_of_probs.items(), key=lambda x: -x[1])\n",
    "    top_five_sent = [sent for sent, prob in sorted_dict[:5]]\n",
    "    return top_five_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'at the discipline temperament knowledge on this election day …': 4.795949483785998e-16,\n",
       " 'zone in this election day rt businessinsider americans age of': 2.471330382291613e-19,\n",
       " 'this election cycle … i got to look fact did': 1.1045191043819975e-18,\n",
       " 'earn the people like this election cycle of the greed': 2.5495279556177294e-18,\n",
       " 'this afternoon phone bank on this election days should be': 1.5494765107522896e-16,\n",
       " 'confirmation process as a party leadership in this election is': 1.2631291616645124e-18,\n",
       " 'government must end election in early and share yours have': 0.0,\n",
       " 'force against this election days left to be a trump': 9.123987574441706e-18,\n",
       " 'a country can fix this election will make the us-mexico': 8.327649111661496e-20,\n",
       " 'to solve this election join forces that some questions at': 1.5726586448343183e-19,\n",
       " 'this election days left to run interference from the wealthy': 5.614198440014693e-16}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_of_probs = sent_prob(filtered_sent, cpd_1gram, cpd_2gram)\n",
    "dict_of_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_five = get_top_five(dict_of_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top 5 tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this election days left to run interference from the wealthy\n",
      "=============\n",
      "at the discipline temperament knowledge on this election day …\n",
      "=============\n",
      "this afternoon phone bank on this election days should be\n",
      "=============\n",
      "force against this election days left to be a trump\n",
      "=============\n",
      "earn the people like this election cycle of the greed\n",
      "=============\n"
     ]
    }
   ],
   "source": [
    "for tweet in top_five:\n",
    "    print(tweet)\n",
    "    print('=============')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
